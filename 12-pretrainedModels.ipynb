{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf6fbd-54c3-4979-8987-52b0d2ff7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7376ac-b9f3-4932-9afb-d5158c0d82f5",
   "metadata": {},
   "source": [
    "# Models and pre-trained weights\n",
    "[Additional Reading](https://pytorch.org/vision/stable/models.html)\n",
    "\n",
    "The torchvision.models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection, video classification, and optical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db4340-c301-48e6-8085-dff6e37c5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "dir(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0bee4-a58c-489a-b775-3a105d3e4737",
   "metadata": {},
   "source": [
    "### Initializing pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57cabb-568b-4fde-8cb6-b446b2eb7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you get ssl error run following\n",
    "#import ssl\n",
    "#ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "resnet.eval() # we will be using the model for evaluation not training. This step is important as some models have different behaviour at traing and eval time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301700d3-3801-4f61-b96e-b2757ada4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=models.ResNet50_Weights.DEFAULT\n",
    "weights.meta[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea52b3-f155-4a3e-8e22-529edb49416d",
   "metadata": {},
   "source": [
    "### Using the pre-trained models\n",
    "Before using the pre-trained models, one must preprocess the image (resize with right resolution/interpolation, apply inference transforms, rescale the values etc). There is no standard way to do this as it depends on how a given model was trained. It can vary across model families, variants or even weight versions. Using the correct preprocessing method is critical and failing to do so may lead to decreased accuracy or incorrect outputs.\n",
    "\n",
    "All the necessary information for the inference transforms of each pre-trained model is provided on its weights documentation. To simplify inference, TorchVision bundles the necessary preprocessing transforms into each model weight. These are accessible via the weight.transforms attribute:\n",
    "\n",
    "[Reference](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c27d3-b494-44b0-a958-1f1312757e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Transforms (preprocessing)\n",
    "preprocess = models.ResNet50_Weights.DEFAULT.transforms()\n",
    "\n",
    "# Read image using Pytorch read_image\n",
    "img = read_image('./data/images/motorbike.jpeg')\n",
    "\n",
    "### or get the image from internet using the following:\n",
    "#from PIL import Image\n",
    "#import requests\n",
    "#img = Image.open(requests.get('http://farm8.staticflickr.com/7090/7399887950_8845d3e6e4_z.jpg', stream=True).raw)\n",
    "\n",
    "\n",
    "# Apply the preprocessing to the input image. Preprocess accepts Pytorch tensor or PIL image\n",
    "batch = preprocess(img).unsqueeze(0) # unsqueeze will reshape the tensor to the correct shape\n",
    "\n",
    "# Apply the model to the image\n",
    "prediction = resnet(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score}%\")\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(transforms.ToPILImage()(img))\n",
    "#plt.imshow(img) #if the img is already a PIL image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b87297-b135-4367-8e92-2692a4d6ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f9511b-fb89-4ab0-b452-c7b641743844",
   "metadata": {},
   "source": [
    "## Fine tuning\n",
    "Rather than training from scratch, the preferred technique is transfer learning, achieved by fine-tuning pre-trained models on custom datasets. By following this approach we use their existing knowledge and tailor them to our specific tasks, thereby conserving significant time and computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f6a51-f7f2-4f67-bd49-0377dcb19897",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 # let say we want to finetune resnet on a dataset containing 10 classes\n",
    "\n",
    "# Load pre-trained model \n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "preprocess = models.ResNet50_Weights.DEFAULT.transforms()\n",
    "\n",
    "# Freeze layer weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Modify the model head for fine-tuning\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes),\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f52f6c-2711-4c6f-9ee9-f643b5f13945",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              preprocess])\n",
    "trainset = datasets.CIFAR10(\n",
    "    root='~/Downloads/',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "testset = datasets.CIFAR10(\n",
    "    root='~/Downloads/',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batchsize = 64\n",
    "trainloader = DataLoader(trainset, batch_size=batchsize, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab216c5-b521-4184-9312-746a353d3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_history = []\n",
    "val_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d303f0-916f-4929-97c3-fe8cc3a30dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.to(device)\n",
    "model.train() # tell the model that your are trainin the model\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criteria(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criteria(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    print(f'Epoch [{epoch}], train loss: {train_loss/len(trainset)}, val loss: {val_loss/len(testset)}')        \n",
    "    train_history += [train_loss/len(trainset)]\n",
    "    val_history += [val_loss/len(testset)]\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1afa814-0ffe-4ead-a54a-13e05350f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_history, 'b')\n",
    "plt.plot(val_history, 'r')\n",
    "plt.title('Convergence plot of gradient descent')\n",
    "plt.xlabel('No of Epochs')\n",
    "plt.ylabel('J')\n",
    "plt.legend('train loss', 'val loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707a6c3-6e00-4e4b-a0a7-71c82427d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Normalize((-1.,-1.,-1.),(2.,2.,2.)), \n",
    "                            transforms.ToPILImage()])\n",
    "\n",
    "idx_to_class = {value: key for key, value in trainset.class_to_idx.items()}\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "    \n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "plt.figure(figsize=(20,30))\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(transform_back(images[i]))\n",
    "    plt.axis('off')\n",
    "    plt.title(idx_to_class[predicted[i].item()])\n",
    "    print(f\"Actual {idx_to_class[labels[i].item()]}\\tPredicted: {idx_to_class[predicted[i].item()]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65bc32b-7dc3-4f9e-9f4d-e4b88169a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4dc9b-373f-4268-a871-dad2d74832f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1b9cb-b64a-44e2-a01c-e1f41e03e261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
